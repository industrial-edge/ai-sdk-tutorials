{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Object Detection Model\n",
        "\n",
        "This notebook explains how the model is designed and how to use it.  \n",
        "\n",
        "The model based on a [Faster RCNN model with a ResNet-50-FPN backbone](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn). The input to the model is a list of tensors, each of shape [Color, Width, Height], one for each image.  \n",
        "\n",
        "The trained model is able to detect circles and lines on an image, in this case these represent holes and scratches on a picture of a piece of work from the shopfloor. This way a postprocessing step is able to decide if the piece is intact and ready for further processing.  \n",
        "The images for testing the model can be produced with notebook [01-CreateTestSet.ipynb](./01-CreateTestSet.ipynb).  \n",
        "The model is placed in the `models` folder with name [`object_detection.onnx`](../models/object_detection.onnx)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import onnx\n",
        "model_onnx = onnx.load(\"../models/object_detection.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the model is loaded we can study it with its input and outputs.  \n",
        "As that is shown below, the graph was saved with in input tensor with shape `[1 x 3 x 224 x 224]`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_onnx.graph.input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to the input, we also can explore the output shape of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_onnx.graph.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can spot, that the model will produce three outputs, one for the coordinates of `boxes` found, one for the `labels` which object was found in the box and another outputs with the `scores`, showing how sure the model on the prediction.  \n",
        "We also can spot that their first dimension is not defined, as we don't know, how many object will be spotted on the picture.\n",
        "The case of `GPU Runtime` of `AI Inference Server`, this dimension should be defined with dimension `-1` instead of leaving it undefined.  \n",
        "So the next cell defines these dimensions for `-1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for output in model_onnx.graph.output:\n",
        "    print(\"before\\n\", output.type.tensor_type.shape)\n",
        "    for dim in output.type.tensor_type.shape.dim:\n",
        "        if dim.dim_value == 0:\n",
        "            print(\"now\")\n",
        "            dim.dim_value = -1\n",
        "    print(\"after\\n\", output.type.tensor_type.shape)\n",
        "onnx.save(model_onnx, \"../src/detection/1/model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model for inference\n",
        "\n",
        "If we want to validate the model, we need to initiate an [InferenceSession](https://onnxruntime.ai/docs/api/python/api_summary.html) by [onnxruntime]() module.  \n",
        "This will enable the model to run and produce outputs from the given inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from onnxruntime import InferenceSession\n",
        "\n",
        "session = InferenceSession(\"../src/detection/1/model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input tensors\n",
        "\n",
        "In the code block below we create a method which can visualize the predictions for a single image.  \n",
        "By defining variable `CONFIDENCE` we set a limit, so only the predictions will be visualized where the confidence of the prediction is higher than this value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "from PIL import ImageDraw, ImageFont\n",
        "\n",
        "CLASSES = [ '__background__', 'hole', 'scratch' ]\n",
        "COLORS = list(tuple(x) for x in numpy.random.randint(low=0, high=255, size=(len(CLASSES), 3)))\n",
        "_font = ImageFont.load_default(size=11)\n",
        "\n",
        "_GREEN = (16, 255, 16)  # color for hole boxes\n",
        "_RED = (255, 16, 16)  # color for scratch boxes\n",
        "\n",
        "CONFIDENCE = 0.8\n",
        "\n",
        "def draw_prediction(image, boxes, labels, scores):\n",
        "\tcanvas = ImageDraw.Draw(image)\n",
        "\tfor i in range(0, len(boxes)):\n",
        "\t\t_class = labels[i]\n",
        "\t\tconfidence = scores[i]\n",
        "\t\tif confidence > CONFIDENCE:\n",
        "\t\t\tbox = boxes[i]\n",
        "\t\t\t(startY, startX, endY, endX) = box.astype(\"int\") \n",
        "\t\t\tlabel = \"{}: {:.2f}%\".format(CLASSES[_class], confidence * 100)\n",
        "\t\t\tprint(\"[INFO] {} ({})\".format(label, box))\n",
        "\t\t\tCOLOR = _GREEN if _class == 1 else _RED\n",
        "\t\t\tcanvas.rectangle([(startX, startY), (endX, endY)], outline=COLOR, width=2)\n",
        "\n",
        "\t\t\ty = startY - 4 if startY - 4 > 4 else startY + 4\n",
        "\t\t\tcanvas.text(xy=(startX+4, y), text=label, font=_font)\n",
        "\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By executing the code block below, it will choose images randomly in  number of `samples_size` and then visualize the predictions.  \n",
        "The `CONFIDENCE` threshold can be set as described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "image_dir = Path('../data/processed')\n",
        "\n",
        "images = list(image_dir.rglob(\"./**/*.jpg\"))\n",
        "images_count = len(images)\n",
        "\n",
        "samples_size = 3\n",
        "sample_indexes = set(numpy.random.randint(0, images_count - 1, (samples_size))) \n",
        "print(\"sample_indexes\", sample_indexes)\n",
        "\n",
        "CONFIDENCE = 0.5\n",
        "\n",
        "sample_images = []\n",
        "for idx in sample_indexes:\n",
        "    print(f\"loading image '{images[idx]}'..\")\n",
        "    pil_image = Image.open(images[idx])\n",
        "    image_array = numpy.array(pil_image).astype('float32').transpose(2,1,0)\n",
        "    image_array /= 255.\n",
        "    \n",
        "    session = InferenceSession(\"../src/detection/1/model.onnx\")\n",
        "    boxes, labels, scores = session.run([\"boxes\",\"labels\",\"scores\"], {\"input\": numpy.array([image_array])})\n",
        "    draw_prediction(pil_image, boxes, labels, scores)\n",
        "    pil_image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are ready to create \n",
        "\n",
        "- a `Preprocessing` step to create the `input` for the GPURuntimeComponent, and\n",
        "- a `Postprocessing` step to process the `output` of the GPURuntimeComponent  \n",
        "\n",
        "in notebook [20-PreAndPostProcessing.ipynb](./20-PreAndPostProcessing.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "(Python) Object Detection",
      "language": "python",
      "name": "object_detection"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}