{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "SPDX-FileCopyrightText": "Copyright (C) Siemens AG 2021. All Rights Reserved.",
		    "SPDX-License-Identifier": "MIT"
      },
      "source": [
        "# Create the inference wrapper\n",
        "In order to use the trained model, you need to write a wrapper script that loads the model and feeds it the data coming from the configured data source.\n",
        "An example is already implemented under the `src` folder and in the `entrypoint.py` file. This notebook helps you to write such a wrapper, and understand how it works.\n",
        "\n",
        "To execute this notebook, you need:\n",
        "- the model created in notebook [10-CreateClassificationModel](10-CreateClassificationModel.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model\n",
        "The wrapper script must load the model first.\n",
        "The filesystem layout (python scripts in the `src` folder, model in the `models` folder) is reproduced during pipeline run,\n",
        "so you can run the same code while experimenting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "%matplotlib inline\n",
        "\n",
        "model_path = Path('../models/classification_mobilnet.tflite').resolve()\n",
        "\n",
        "with open(model_path, 'rb') as rpl:\n",
        "    model = rpl.read()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=model)\n",
        "interpreter.allocate_tensors()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feed image into the model\n",
        "Once the model is loaded it is ready to make predictions but some preprocessing of the images is still needed. The following code bit shows an example of wrapping the preprocessing and the predicting steps together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "SCALE = 255\n",
        "\n",
        "# Define a method to wrap the preprocessing of the image and the model making a prediction\n",
        "def predict_from_image(image):\n",
        "    \n",
        "    input_arr = np.array(image)*1/SCALE\n",
        "    assert input_arr.shape == (IMAGE_WIDTH, IMAGE_HEIGHT, 3), \"The input image must contain RGB channels without alpha and must be of a certain size.\"\n",
        "    input_arr = np.array([input_arr], dtype=np.float32)  # Convert single image to a batch.\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_arr)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    index = np.argmax(predictions, axis=-1).item()\n",
        "    return index, float(predictions[0][index])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try it with an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "image_dir = Path('../data/processed')\n",
        "\n",
        "images = list(str(f) for f in image_dir.rglob(\"*.jpg\"))\n",
        "images_count = len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "image = cv2.imread(images[0])  # BGR image 224x224x3\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # RGB image 224x224x3\n",
        "image = cv2.resize(image, (224, 224))\n",
        "\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict_from_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create ImageSet payload format\n",
        "\n",
        "The method defined below creates the required format, and will be put into the payload with name `vision_payload` which will be provided by the `AI Inference Server` when an image is arrived from the selected camera through `Vision Connector Application`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_to_bayer(image_path):\n",
        "    im = cv2.imread(image_path)  \n",
        "    im = cv2.resize(im, (224, 224)) # RGB image 224x224x3\n",
        "    (height, width) = im.shape[:2]\n",
        "    (R,G,B) = cv2.split(im)\n",
        "\n",
        "    bayerrg8 = np.zeros((height, width), np.uint8)\n",
        "\n",
        "    # strided slicing for this pattern:\n",
        "    #   R G\n",
        "    #   G R\n",
        "    bayerrg8[0::2, 0::2] = R[0::2, 1::2] # top left\n",
        "    bayerrg8[0::2, 1::2] = G[0::2, 0::2] # top right\n",
        "    bayerrg8[1::2, 0::2] = G[1::2, 1::2] # bottom left\n",
        "    bayerrg8[1::2, 1::2] = B[1::2, 0::2] # bottom right\n",
        "\n",
        "    return bayerrg8, width, height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def create_imageset_dict(image_path):\n",
        "    timestamp = datetime.datetime.now()\n",
        "    \n",
        "    image_bytes, width, height = image_to_bayer(image_path)\n",
        "\n",
        "    return {\n",
        "        \"version\": \"1\",  # version of the Metadata format\n",
        "        \"count\": 1,  # Number of images on message\n",
        "        \"timestamp\": timestamp.isoformat(),  # Camera acquisition time\n",
        "        \"detail\": [{  # list of images with detailed information\n",
        "            \"id\": str(image_path),  # unique image identifier. this case we use the filename of the original image\n",
        "            \"timestamp\": str(timestamp.timestamp()),  # Timestamp provided by the camera\n",
        "            \"width\": width,  # image width\n",
        "            \"height\": height,  # image height\n",
        "            \"format\": \"BayerRG8\",  # image format configure\n",
        "            \"metadata\": \"\",  # optional extra information on image\n",
        "            \"image\": image_bytes  # image binary with the given 'format'\n",
        "        }]\n",
        "    }\n",
        "\n",
        "image_set_payload = {\"vision_payload\": create_imageset_dict(images[0])}\n",
        "image_set_payload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract image from ImageSet\n",
        "\n",
        "Now the task of the PythonComponent is to extract image data from the payload, and creates a flat array of float32 data.  \n",
        "This time the original image is packaged into the payload in `BayerRG8` format, so the Python component converts it to a one-dimensional float32 array with using PIL Image and numpy transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extracted = image_set_payload['vision_payload']\n",
        "image_detail = extracted[\"detail\"][0]\n",
        "\n",
        "iuid = image_detail['id']\n",
        "width = image_detail.get(\"width\", 224)\n",
        "height = image_detail.get(\"height\", 224)\n",
        "\n",
        "image_data = np.frombuffer(image_detail['image'], dtype=np.uint8)  # BayerRG8, (width x height, )\n",
        "print(image_data.shape)\n",
        "image_data = image_data.reshape(width, height)                           # BayerRG8, (width, height)\n",
        "print(image_data.shape)\n",
        "image_data = cv2.cvtColor(image_data, cv2.COLOR_BayerRG2RGB)             # RGB, (width, height, 3)\n",
        "print(image_data.shape)\n",
        "\n",
        "image_array = image_data.astype(np.float32) / 255.  # normalizing into [0,1) range and converting to float32\n",
        "\n",
        "print(f\"image_array shape: {image_array.shape}\")  # checking the image dimensions\n",
        "plt.imshow(image_array)  # showing the image\n",
        "plt.axis('off')\n",
        "\n",
        "image_array = image_array.transpose(2,0,1)  # changing the shape from (224, 224, 3) to (3, 224, 224) as expected by the model\n",
        "inputs = np.array(image_array.ravel())  # flattening the 3 dimensional array and adding to an empty batch\n",
        "print(f\"inputs shape {inputs.shape} and type '{inputs.dtype}'\")  # checking the input shape and type"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create entrypoint"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we are ready to wrap everything together in an entrypoint script that the AI Inference Server can execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_input(data: dict):\n",
        "    extracted = data['vision_payload']\n",
        "    image_detail = extracted[\"detail\"][0]\n",
        "\n",
        "    iuid = image_detail['id']\n",
        "    width = image_detail.get(\"width\", 224)\n",
        "    height = image_detail.get(\"height\", 224)\n",
        "\n",
        "    image_data = np.frombuffer(image_detail['image'], dtype=np.uint8)\n",
        "    image_data = image_data.reshape(width, height)                         \n",
        "    \n",
        "    image_data = cv2.resize(image_data, (224, 224))\n",
        "    image = cv2.cvtColor(image_data, cv2.COLOR_BayerRG2RGB)\n",
        "\n",
        "    if image is None:\n",
        "        return None\n",
        "    \n",
        "    prediction, _ = predict_from_image(image)\n",
        "\n",
        "    return {\"prediction\": str(prediction)}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try it with an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "test_inference_wrapper_entrypoint"
        ]
      },
      "outputs": [],
      "source": [
        "image_set_payload = create_imageset_dict('../data/processed/simatic_photos/S7_1500/IMG_1651.JPG')\n",
        "data = { \"vision_payload\": image_set_payload }\n",
        "\n",
        "print(process_input(data))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics and Object Output\n",
        "\n",
        "You can return multiple outputs, and some of those outputs can serve as custom metrics for your running model.\n",
        "\n",
        "When returning such custom metric outputs, the int or float value must be serialized in a specific way, like in the example below.\n",
        "\n",
        "The metrics have to be defined separately from the outputs when creating a pipeline component.\n",
        "\n",
        "The latest versions of AI Inference Servers support an internal variable type which can be transferred between pipeline steps or can be passed to a Zero Message Queue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def process_input(data: dict):\n",
        "\n",
        "    extracted = data['vision_payload']\n",
        "    image_detail = extracted[\"detail\"][0]\n",
        "\n",
        "    width = image_detail.get(\"width\", 224)\n",
        "    height = image_detail.get(\"height\", 224)\n",
        "\n",
        "    image_data = np.frombuffer(image_detail['image'], dtype=np.uint8)\n",
        "    image_data = image_data.reshape(width, height)                         \n",
        "    \n",
        "    image_data = cv2.resize(image_data, (224, 224))\n",
        "    image = cv2.cvtColor(image_data, cv2.COLOR_BayerRG2RGB)\n",
        "    \n",
        "    if image is None:\n",
        "        return None\n",
        "\n",
        "    prediction, ic_probability = predict_from_image(image)\n",
        "\n",
        "    return {\n",
        "        \"prediction\": str(prediction),\n",
        "        \"ic_probability\": metric_output(ic_probability)\n",
        "    }\n",
        "\n",
        "def metric_output(v: float):\n",
        "    return json.dumps({\"value\": v})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = process_input(data)\n",
        "\n",
        "print(\"prediction:\", result[\"prediction\"])\n",
        "print(\"probability:\", result[\"ic_probability\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (image_classification)",
      "language": "python",
      "name": "image_classification"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}