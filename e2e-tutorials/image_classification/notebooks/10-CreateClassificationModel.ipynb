{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "copyright": "Copyright (C) Siemens AG 2021. All Rights Reserved."
            },
            "source": [
                "# Train a Classification Model\n",
                "\n",
                "In this notebook we create and train a model which is assumed to infer on 'AI Inference Server'.  \n",
                "The model itself is the well known `Mobilnet` classifier network pre-trained for images, and at the end of this notebook we extend it with layers and train it to be able to classify our example images.  \n",
                "The notebook [20-CreateInferenceWrapper](20-CreateInferenceWrapper.ipynb) will show how to wrap the model into Python code and which in- and outputs should be handled.  \n",
                "The notebooks with number 30 will show how to create an 'Edge Configuration Package' to be imported directly on 'AI Inference Server'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import tensorflow as tf\n",
                "\n",
                "%matplotlib inline\n",
                "print('tensorflow: {}'.format(tf.__version__))\n",
                "\n",
                "sys.path.append('../src')\n",
                "import utils\n",
                "\n",
                "PYTHON_VERSION = sys.version_info"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Download dataset \n",
                "\n",
                "To train a model, an image set is required.  \n",
                "Such an image set is available in the repository of [Simatic AI Launcher](https://github.com/siemens/simatic-ai-launcher).\n",
                "In the next cell, and the `tf.keras.utils.get_file(..)` command will download and extract the tar-zipped file into the `../data/processed` folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_name = tf.keras.utils.get_file(\n",
                "    fname=Path('../data/raw/simatic_photos.tgz').resolve(),\n",
                "    origin='https://github.com/siemens/simatic-ai-launcher/raw/master/datasets/simatic_photos.tgz',\n",
                "    extract=True, cache_dir=Path('../data/processed/').resolve(), cache_subdir=\"\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Retrain an existing Convolutional Neural Network"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create a dataset for training"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `../data/processed/simatic_photos` folder is created in the previous step and contains one folder for every class to be identified. The images belonging to a class are within the corresponding folder.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "image_dir = Path('../data/processed/simatic_photos')\n",
                "\n",
                "class_labels = [path.name for path in image_dir.iterdir() if path.is_dir()]\n",
                "class_labels.sort()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "IMAGE_WIDTH = 224\n",
                "IMAGE_HEIGHT = 224\n",
                "IMAGE_SIZE = (IMAGE_WIDTH,IMAGE_HEIGHT)\n",
                "SCALE = 255\n",
                "\n",
                "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/SCALE, validation_split=0.2)\n",
                "training_set = image_generator.flow_from_directory(str(image_dir), target_size=IMAGE_SIZE, subset='training')\n",
                "validation_set = image_generator.flow_from_directory(str(image_dir), target_size=IMAGE_SIZE, subset='validation')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Retrieve MobileNet network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# in this command result deprecation warning can be shown up, but it is normal\n",
                "dim = 3\n",
                "feature_extractor = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE+(dim,))) \n",
                "feature_extractor.trainable=False\n",
                "print(f'Type of feature extractor: {type(feature_extractor)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Build the model "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "image_batch, label_batch = next(iter(training_set))\n",
                "\n",
                "x = feature_extractor.output\n",
                "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
                "# Uncomment to increase accuracy with additional hidden alyers:\n",
                "# x = tf.keras.layers.Dense(label_batch.shape[1] ** 2, activation='relu')(x)\n",
                "classifier = tf.keras.layers.Dense(label_batch.shape[1], activation='softmax')(x) # final layer with softmax activation\n",
                "\n",
                "model = tf.keras.Model(inputs=feature_extractor.input,outputs=classifier)\n",
                "    \n",
                "model.build((None,)+IMAGE_SIZE+(3,))\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prepare for training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.compile(\n",
                "  optimizer=tf.keras.optimizers.Adam(),\n",
                "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
                "  metrics=['acc'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = model.fit(training_set, epochs=4,\n",
                "                              steps_per_epoch=training_set.num_batches,\n",
                "                              validation_data=validation_set,\n",
                "                              callbacks = [])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Check predictions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we are able to check the accuracy of the trained model by plotting its confusion matrix.  \n",
                "Please note that this plot only checks the first batch of the validation set.  \n",
                "If everything went well, we are done, and we have a trained model we can use as the neural network in the AI Template."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "image_batch, label_batch = validation_set[0]\n",
                "predictions = model.predict(image_batch)\n",
                "\n",
                "utils.show_confusion(label_batch, predictions, class_labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Save model\n",
                "\n",
                "This model can be used on TM-NPU and on AI Inference Server with TensorFlow as well."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save(Path('../models/classification_mobilnet.h5'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Convert model to TensorFlow lite\n",
                "\n",
                "This version of the model can be used on AI Inference Server with TFLite and results in a more lightweight pipeline."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Please note, there is no official TFLite runtime for Windows.\n",
                "\n",
                "You can either use the full TensorFlow package, or build the TFLite wheel manually, following this guide: https://www.tensorflow.org/lite/guide/build_cmake_pip"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf2\n",
                "\n",
                "# Convert the model.\n",
                "converter = tf2.lite.TFLiteConverter.from_keras_model(model)\n",
                "tflite_model = converter.convert()\n",
                "\n",
                "# Save the model.\n",
                "with open('../models/classification_mobilnet.tflite', 'wb') as f:\n",
                "  f.write(tflite_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The saved model can be used in notebook [30-CreatePipelinePackage](30-CreatePipelinePackage.ipynb) to create a deployment package."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "image-classification",
            "language": "python",
            "name": "image-classification"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
