{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "copyright": "Copyright (C) Siemens AG 2021. All Rights Reserved."
      },
      "source": [
        "# Create a trained model\n",
        "\n",
        "This notebook shows the necessary steps you need to take in order to train and save a model if you already know the most appropriate parameters.  \n",
        "At the end of the notebook, you will have a trained model which can be delivered to the AI Inference Server in a properly formed _'Edge configuration package'_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"../src\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the training data\n",
        "\n",
        "The `../data/raw/example.zip` is an example dataset for training a state identifier model. It contains `json` files holding labeled timeseries data in batches of 300.\n",
        "\n",
        "The `../data/processed/` folder is a convenient place to upack the dataset, and then load it into a list of dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "data_path = Path('../data/processed/example')\n",
        "\n",
        "shutil.rmtree(data_path, ignore_errors=True)\n",
        "with zipfile.ZipFile(\"../data/raw/example.zip\", 'r') as zip_file:\n",
        "    zip_file.extractall(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the model works with numpy arrays, the incoming data and the expected predictions from the `DataFrame`s have to be transformed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "data_path = Path('../data/processed/example')\n",
        "\n",
        "dataframes = []\n",
        "for json_file in data_path.glob('*.json'):\n",
        "    with open(json_file) as f:    \n",
        "        data = json.load(f)\n",
        "        dataframe = pandas.json_normalize(data, \"measurements\", [\"class\"])\n",
        "        dataframes.append(dataframe)\n",
        "\n",
        "X = np.array([x[[\"ph1\",\"ph2\",\"ph3\"]].values for x in dataframes])\n",
        "Y = np.array([y[\"class\"].values[0] for y in dataframes])\n",
        "print(\"Shape:\", X.shape)\n",
        "print(\"Dimensions:\", X.ndim)\n",
        "print(\"Labels:\", Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use a list of feature extractors to preprocess the input data, extracting the required features for the `KNeighborsClassifier` classifier.\n",
        "\n",
        "The parameters may need to be tuned for the current usecase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tsfresh.feature_extraction.feature_calculators as fc\n",
        "from si.preprocessing import positive_sum_of_changes, negative_sum_of_changes, FillMissingValues, SumColumnsTransformer\n",
        "\n",
        "weighted_feature_list = [\n",
        "    (2, [ fc.maximum, fc.minimum, fc.mean ]),\n",
        "    (1, [ fc.variance, fc.standard_deviation ]),\n",
        "    (1, [ fc.sum_values ]),\n",
        "    (1, [ fc.absolute_sum_of_changes ]),\n",
        "    (1, [ positive_sum_of_changes, negative_sum_of_changes ]),\n",
        "    (1, [ fc.count_above_mean, fc.longest_strike_above_mean,  fc.longest_strike_below_mean ])\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the feature extractors, create a `scikit-learn` pipeline defining two steps:\n",
        "- `preprocess`:\n",
        "  - fills missing values, \n",
        "  - summarizes the three input feature, \n",
        "  - extracts the required features of time window\n",
        "  - normalizes the extracted features\n",
        "- `classify`:\n",
        "  - use the trained model to classify incoming data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from si.pipeline import FeatureTransformer\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocess', Pipeline([\n",
        "        ('fillmissing', FillMissingValues('ffill')),\n",
        "        ('summarize', SumColumnsTransformer()), # summarizes the variables into one variable\n",
        "        ('featurize', FeatureTransformer(function_list=weighted_feature_list)),\n",
        "        ('scale', MinMaxScaler(feature_range=(0, 1)))])),\n",
        "    ('classify', KNeighborsClassifier(n_neighbors=3)),\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the two pipeline steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model[\"preprocess\"].fit(X)\n",
        "y = model[\"preprocess\"].transform(X)\n",
        "model[\"classify\"].fit(y, Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try out the classification!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction = model[\"classify\"].predict(model[\"preprocess\"].transform(X))\n",
        "print(\"Training labels:\")\n",
        "print(Y)\n",
        "print(\"Prediction:\")\n",
        "print(prediction)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the model\n",
        "\n",
        "If you are satisfied with the result, you can save the trained model as a joblib file.  \n",
        "You will need this later on to create a pipeline configuration package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "model_path = f\"../models/bsi-model.joblib\"\n",
        "with open(model_path, 'wb') as fh:\n",
        "    joblib.dump(model, model_path, compress=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subsequent notebooks\n",
        "\n",
        "Notebook [20-CreateInferenceWrapper](20-CreateInferenceWrapper.ipynb) shows you how to create a Python wrapper around the model.  \n",
        "Notebook [30-CreatePipelinePackage](30-CreatePipelinePackage.ipynb) demonstrates how to create the pipeline configuration package. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": " Python (batch_state_identifier)",
      "language": "python",
      "name": "batch_state_identifier"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
