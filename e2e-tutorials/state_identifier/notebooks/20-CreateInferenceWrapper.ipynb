{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "copyright": "Copyright (C) Siemens AG 2021. All Rights Reserved."
      },
      "source": [
        "# Create the inference wrapper\n",
        "\n",
        "In order to use the trained model, you must write a wrapper script that loads the model and feeds it with the data coming from the configured data source. This notebook helps you to write such a wrapper, and understand how it works.\n",
        "\n",
        "To execute this notebook, we will need:\n",
        "- training data set which was used in notebook [10-CreateClusteringModel](10-CreateClusteringModel.ipynb)\n",
        "- the model created in the above notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the model\n",
        "\n",
        "The wrapper script must first load the model into the memory in order to make the prediction as fast as possible.  \n",
        "The filesystem layout (python scripts in `src` folder, model in the `models` folder) is reproduced when running the pipeline, so you can run the same code while experimenting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path('../src').resolve()))\n",
        "\n",
        "model_path = Path('../models/clustering-model.joblib').resolve()\n",
        "\n",
        "with open(model_path, 'rb') as rpl:\n",
        "    pipe = joblib.load(rpl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract model metadata \n",
        "\n",
        "Beside the saved model we also included some metadata in the joblib file. We extract them to the following global variables:\n",
        "- `pipe`: the trained Scikit-Learn Pipeline itself\n",
        "- `input_columns`: the name of the pipeline inputs\n",
        "- `output_name`: the name of the pipeline output\n",
        "- `window`- and `step_size`: the windows size and the frequency of window creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "input_columns = [\"ph1\", \"ph2\", \"ph3\"]\n",
        "output_name = \"prediction\"\n",
        "# set model parameters\n",
        "window_size = pipe.get_params().get('preprocessing__windowing__window_size')\n",
        "step_size = pipe.get_params().get('preprocessing__windowing__window_step')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Input data\n",
        "\n",
        "AI Inference Server wraps the acquired input values into a dictionary and passes them to `process_input(data: dict)` in this form as a single parameter. Each input variable is represented as a separate element in the dictionary. In the code cell below we create the same format from the well known training data and print the first 10 element of this list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas\n",
        "\n",
        "data_files = glob.glob(\"../data/raw/*.csv\")\n",
        "input_series = pandas.read_csv(data_files[0])  # read test data from the same csv used to train the model\n",
        "input_list = input_series[['ph1', 'ph2', 'ph3']].to_dict(orient='records')  # creating a list of dictionaries as the `process_input(..)` method receives them\n",
        "input_list[:10]  # show the first 10 element of the list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feed window of data into the model\n",
        "\n",
        "Now we can create a method that utilizes the model and produces a prediction from a window of input rows according to `window_size`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model: dict, model_input: numpy.array):\n",
        "    prediction = model.predict(model_input)\n",
        "\n",
        "    return prediction[0]\n",
        "\n",
        "# Now we can emulate what happens if the data is collected\n",
        "predict(pipe, input_list[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Process incoming data and aggregate a window\n",
        "\n",
        "First we set up a buffer array named `aggregated_data` to store the incoming data until the `window_size` is reached.\n",
        "\n",
        "Then we define `process_data(input_dict: dict)`, which we will call for every row of input data. It extracts the expected input variables from the dictionary to be received from AI Inference Server and aggregates input rows into `aggregated_data`. The function returns `None` while input is only accumulated but the window size was not reached.\n",
        "\n",
        "Once the window size is reached, the function produces a prediction and returns it in a dictionary using `output_name` as key. This dictionary can then be passed directly back to AI Inference Server. When a prediction is produced for a data window, we also need to remove the first `step_size` elements from `aggregated_data` to prepare forming the next window using subsequent input data.\n",
        "\n",
        "This function implements the signature of an entrypoint. The only reason why it does not qualify as an entrypoint is due to the current technical limitation that the entrypoint must be located in the root folder of the pipeline component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create an aggregation array for incoming data\n",
        "aggregated_data = numpy.empty((0, len(input_columns)), int)\n",
        "\n",
        "def process_data(input_dict: dict):\n",
        "    global aggregated_data\n",
        "    \n",
        "    values = [[numpy.nan if input_dict[variable] is None else input_dict[variable] for variable in input_columns]]\n",
        "    aggregated_data = numpy.append(aggregated_data, values, axis=0)\n",
        "\n",
        "    if len(aggregated_data) >= window_size:\n",
        "        output = {output_name: predict(pipe, aggregated_data)}\n",
        "        aggregated_data = aggregated_data[step_size:]\n",
        "        return output\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can emulate in iteration that the method is called 300 times with the expected payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "test_inference_wrapper_entrypoint"
        ]
      },
      "outputs": [],
      "source": [
        "for i in range(300):\n",
        "    result = process_data(input_list[i])\n",
        "    if result is not None: print(i, result) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create entrypoint\n",
        "\n",
        "The entrypoint can in this case simply delegate to the function defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_input(data: dict):\n",
        "\n",
        "    return process_data(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try it with an example\n",
        "\n",
        "With each input `process_input(...)` will be triggered. The function will return `None` until `window_size` is reached. When the input completes a window, the function gives back a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "output_list = []\n",
        "for input_data in input_list[:window_size]:\n",
        "    output_list.append(process_input(input_data))\n",
        "\n",
        "[output for output in output_list if output]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Allow parameter updates\n",
        "\n",
        "AI Inference Server allows you to change predefined parameters in your pipeline at runtime. This way you can set your variables while the pipeline is running. For example, we can change our `step_size` to produce predictions more frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_parameters(params: dict):\n",
        "    global step_size\n",
        "    step_size = params.get(\"step_size\", step_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the sake of example, we will calculate predictions for 900 datapoints, with the original step size of 300. That means we will have 3 predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_list = []\n",
        "for input_data in input_list[:window_size*3]:\n",
        "    output_list.append(process_input(input_data))\n",
        "\n",
        "[output for output in output_list if output]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we change the step size to 100, and calculate predictions for the same 900 datapoints, we will get 7 predictions, because we need 300 datapoints for the first window, and then we produce a new prediction after each 100th datapoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(300 / 300 ) + (900 - 300) / 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "update_parameters({'step_size': 100})\n",
        "\n",
        "output_list = []\n",
        "for input_data in input_list[:window_size*3]:\n",
        "    output_list.append(process_input(input_data))\n",
        "\n",
        "[output for output in output_list if output]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Return metrics\n",
        "\n",
        "You can return multiple outputs, and some of those outputs can serve as custom metrics for your running model.\n",
        "\n",
        "When returning such custom metric outputs, the int or float value must be serialized in a specific way, like in the example below.\n",
        "\n",
        "The metrics have to be defined separately when creating a pipeline component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy\n",
        "\n",
        "aggregated_data = numpy.empty((0, len(input_columns)), int)\n",
        "\n",
        "def process_data(input_dict: dict):\n",
        "    global aggregated_data\n",
        "    \n",
        "    values = [[numpy.nan if input_dict[variable] is None else input_dict[variable] for variable in input_columns]]\n",
        "    aggregated_data = numpy.append(aggregated_data, values, axis=0)\n",
        "\n",
        "    if len(aggregated_data) >= window_size:\n",
        "        output = {output_name: predict(pipe, aggregated_data)}\n",
        "        features = pipe[\"preprocessing\"].transform(aggregated_data)[0]\n",
        "        output[\"model_input_min\"]  = metric_output(features[0])\n",
        "        output[\"model_input_max\"]  = metric_output(features[1])\n",
        "        output[\"model_input_mean\"] = metric_output(features[2])\n",
        "        aggregated_data = aggregated_data[step_size:]\n",
        "        return output\n",
        "\n",
        "    return None\n",
        "\n",
        "def metric_output(v: int or float):\n",
        "    return json.dumps({\"value\": v})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(300):\n",
        "    result = process_data(input_list[i])\n",
        "    if result is not None: print(i, result) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the inference wrapper runs the way you desire, you can update the code in [entrypoint.py](../entrypoint.py) and [inference.py](../src/si/inference.py) to match yours, and create an edge package in notebook [30-CreatePipelinePackage](30-CreatePipelinePackage.ipynb)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (state_identifier)",
      "language": "python",
      "name": "state_identifier"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
