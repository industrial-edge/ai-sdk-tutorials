{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "copyright": "Copyright (C) Siemens AG 2021. All Rights Reserved."
      },
      "source": [
        "# Create an edge configuration package \n",
        "\n",
        "In this notebook, the main goal is to create a pipeline with all of the contents that are necessary for the execution of the model on an Industrial Edge device.  \n",
        "In order to put the elements together, this example collects files\n",
        "\n",
        "from [10-CreateClusteringModel](10-CreateClusteringModel.ipynb) notebook:  \n",
        "- **clustering-model.joblib**: the model itself, created in  notebook.\n",
        "\n",
        "from [20-CreateInferenceWrapper](20-CreateInferenceWrapper.ipynb) notebook:  \n",
        "- **entrypoint.py**: the script that is called by the runtime to execute the model on the Edge side\n",
        "- **src** folder: all the other Python scripts necessary to execute the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from simaticai import deployment\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path('../src').resolve()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create an inference component\n",
        "\n",
        "In the [10-CreateClusteringModel](10-CreateClusteringModel.ipynb) notebook we created our model with the proper parameters and trained network. In this step, we create a `PythonComponent` from the saved model.  \n",
        "We need to\n",
        "- include the model in the package as a resource file\n",
        "- define the input variables\n",
        "- define an output variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COMPONENT_DESCRIPTION =\"\"\"\\\n",
        "This component receives data rows of measured energy consumption data (ph1, ph2, ph3) from SIMATIC S7 Connector and predicts a cluster for every 'step_size' number of data rows.\n",
        "\"\"\"\n",
        "\n",
        "# Create a PythonComponent to use the saved model.\n",
        "component = deployment.PythonComponent(name='inference', desc=COMPONENT_DESCRIPTION, version='1.0.0', python_version='3.11')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add entrypoint\n",
        "\n",
        "The entry point acts as an interface between your code and the AI Inference Server. \n",
        "This Python code will unwrap the incoming data to a dictionary for your code and wrap your answer to a formatted response for the Runtime.  \n",
        "The AI Inference Server expects the entry point to be located in the component root directory.  \n",
        "You need to add this file as a resource and set the component's entry point to its file name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "component.add_resources(\"..\", \"entrypoint.py\")\n",
        "component.add_resources(\"..\", \"models/clustering-model.joblib\")\n",
        "component.set_entrypoint('entrypoint.py')  # Defining the entry point which will be triggered through its `run(..)` method"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add Inputs and Output\n",
        "\n",
        "Defining input and output variables of the component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DESCRIPTION1=\"\"\"\\\n",
        "Measured energy consumption on phase 1\n",
        "\"\"\"\n",
        "INPUT_DESCRIPTION2=\"\"\"\\\n",
        "Measured energy consumption on phase 2\n",
        "\"\"\"\n",
        "INPUT_DESCRIPTION3=\"\"\"\\\n",
        "Measured energy consumption on phase 3\n",
        "\"\"\"\n",
        "OUTPUT_DESCRIPTION=\"\"\"\\\n",
        "Predicted cluster of the datapoint (0, 1 or 2)\n",
        "\"\"\"\n",
        "\n",
        "component.add_input(\"ph1\", \"Double\",INPUT_DESCRIPTION1)\n",
        "component.add_input(\"ph2\", \"Double\",INPUT_DESCRIPTION2)\n",
        "component.add_input(\"ph3\", \"Double\",INPUT_DESCRIPTION3)\n",
        "\n",
        "component.add_output(\"prediction\", \"Integer\",OUTPUT_DESCRIPTION)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add metrics\n",
        "It can be useful to monitor the pipeline, e.g. watch how some features change. In this example we expose the minimum, maximum and mean values. The metric name must contain an underscore (`_`), because the part before the underscore is used to group custom metrics on the dashboard.\n",
        "\n",
        "**&#9888; Remember!**\n",
        "You have to use the same names here and in the inference wrapper script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "component.add_metric(\"model_input_min\")\n",
        "component.add_metric(\"model_input_max\")\n",
        "component.add_metric(\"model_input_mean\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add other resources\n",
        "\n",
        "Your code might require additional resources, such as config files, models (as seen above), further Python sources or reference data files.   \n",
        "You can simply add them using `add_resources(base_dir, resources)`.  \n",
        "As base_dir, you have to pass the local directory that you want to correspond to the component root on the runtime. As resources, you can pass a list of paths relative to base_dir. The referred files and directories will be packaged. On the runtime, the resources will be available under the same paths relative to the component root directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "component.add_resources(\"..\", [\"src/si\"])\n",
        "component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a pipeline with the inference component\n",
        "\n",
        "Now you can use the `component` to create an edge configuration package.  \n",
        "During its initialization, it will build the connections between the model and its environment, in particular the wiring of the pipeline inputs and outputs.  \n",
        "The given name and version will form the name of the pipeline and the folder where the necessary configuration files and folder structure will be created.  \n",
        "Also, set the inter-signal alignment periodicity if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "testinit_package_name"
        ]
      },
      "outputs": [],
      "source": [
        "PIPELINE_DESCRIPTION =\"\"\"\\\n",
        "This pipeline runs an Clustering Model on an Industrial Edge device.\n",
        "The aim of the model is to distinguishes 3 operation states of a machine based on its energy consumption.\n",
        "\n",
        "This model was trained by K-Means clustering on energy consumption data measured on 3 phases of electrical current (ph1, ph2, ph3).\n",
        "\n",
        "The pipeline receives data from SIMATIC S7 Connector using AI Inference Server's Inter Signal Alignment feature.\n",
        "The intersignal alignment must be set to the same sampling rate of 250ms that the model was trained on.\n",
        "\n",
        "Data goes through a scikit-learn pipeline consisting of 2 stages, a preprocessing and a clustering.\n",
        "(Please note that even though the scikit pipeline has 2 stages it will be executed as a single component on AI Inference Server)\n",
        "\n",
        "The preprocessing step of the scikit-learn pipeline groups input data rows into data windows, 300 data rows each.\n",
        "This window is advanced according to the 'step_size' parameter.\n",
        "If 'step_size' is set to the window size (300 in this case) the windows will be adjacent.\n",
        "If 'step_size' is set to be smaller than the window size, the windows will overlap.\n",
        "The preprocessing of the scikit-learn pipeline calculates a series of basic statistical features for each window (e.g.: Min, Max, Mean).\n",
        "\n",
        "The model itself is fed with these statistical features, producing a predicted class for each window.\n",
        "As a result the pipeline produces a single output for every 'step_size' number of data rows.\n",
        "The first output is produced after consuming a complete window (300 data rows).\n",
        "\"\"\"\n",
        "\n",
        "#To assure compatibility with older versions of AI SDK (<v1.5.0), you must define the `version` parameter in the `from_components()` method\n",
        "pipeline = deployment.Pipeline.from_components([component], name='State Identifier', desc=PIPELINE_DESCRIPTION)\n",
        "\n",
        "pipeline.add_parameter(\"step_size\", 300, \"Integer\")\n",
        "pipeline.set_timeshifting_periodicity(250)\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make sure everything went as expected, examine the resulting metadata structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(yaml.dump(pipeline.get_datalink_metadata()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define necessary python modules\n",
        "\n",
        "To execute the model with `inference.py` the python runtime environment must contain all the required packages.\n",
        "To do so the AI SDK's `convert_package` method will download the dependencies into the edge configuration package, and here you need to define all the dependencies of your pipeline and model.  \n",
        "While the installation does not execute any post installation script, only those modules can be used which are available in wheel binary format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "testinit_sdk_wheel_path"
        ]
      },
      "outputs": [],
      "source": [
        "pipeline.add_dependencies([\n",
        "    ('tsfresh','0.17.0'), # Runtime needs it to depickle.\n",
        "    ('scikit-learn','1.3.2'),\n",
        "    ('scipy','1.10.1'),\n",
        "    ('statsmodels','0.14.0'),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the edge package\n",
        "\n",
        "This step creates the proper content in the defined target folder and creates the edge configuration package as a zip file.  \n",
        "The `export()` method first validates the pipeline and raises an error if it finds any problems. Manual validation is also possible with the `pipeline.validate()` method.\n",
        "\n",
        "Edge packages are identified by their `package id` and `version` attributes, and will be grouped in AI Inference Server and in other Edge applications by `package id`.\n",
        "When saving a pipeline - in the `export()` method - you can specify a `pacakge id` in a UUID 4 compliant format, or an automatically generated one will be assigned. \n",
        "If no `package id` is defined in the `export()` method, and AI SDK finds an already assigned `package id` in a previously generated, and similarly named package, the `package id` found in the latest package will be used.\n",
        "\n",
        "AI SDK will automatically assign and increase the version number of a pipeline every time a package is saved, unless a new package id is assigned in the `export()` method or an explicit version number is defined without package id either in the `export()` method or in the pipeline constructor.\n",
        "\n",
        "Restrictions: \n",
        "- You can not overwrite a previously saved package with the same `package id` if the `package id` is explicitly assigned in `export()` method \n",
        "- Packages generated with older versions of AI SDK (without `package id`) will be overwritten\n",
        "- If a new `package id` is assigned to an existing version of the package it will overwrite the old one\n",
        "- If no previous version of a package (with a generated or explicitly assigned `package id`) found, AI SDK will assign the version `1` to the created package\n",
        "- Version defined in the `export()` method takes precedence over the version assigned on constructor level\n",
        "\n",
        "Now you are ready to bring your model to the shopfloor by building the edge configuration package. You can achieve this by the following step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "edge_package_path = pipeline.export('../packages')\n",
        "# edge_package_path = pipeline.export(destination='../packages', version=\"1\")\n",
        "# edge_package_path = pipeline.export(destination='../packages', package_id=uuid.uuid4())\n",
        "# edge_package_path = pipeline.export(destination='../packages', package_id=uuid.uuid4(), version=\"1\")\n",
        "edge_package_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the edge configuration package locally\n",
        "\n",
        "We suggest you test your edge configuration package on your computer before you deploy it to an Edge device. It is possible to do so using notebook [40-TestPipelineLocally](40-TestPipelineLocally.ipynb).\n",
        "\n",
        "The package is now ready to be imported on AI Inference Server."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "state_identifier",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
