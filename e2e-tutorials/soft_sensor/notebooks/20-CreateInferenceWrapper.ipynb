{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "copyright": "Copyright (C) Siemens AG 2021. All Rights Reserved."
   },
   "source": [
    "# Create the inference wrapper\n",
    "In order to use the trained model, we need to write a wrapper script that loads the model and feeds it the data coming from the configured data source.  \n",
    "An example is already implemented under the `src` folder and in the `entrypoint.py` file.  \n",
    "This notebook is purely for demonstrating how to write such a wrapper. Execution of this notebook is not expected, and changes being made in this tutorial will not be reflected in the final package!\n",
    "\n",
    "To execute this notebook, you need the model created in notebook [10-CreateClassificationModel](10-CreateClassificationModel.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "The wrapper script must load the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# the model sits in the models folder, so we need to use the relative path for the notebook to find it\n",
    "# this will be changed in entrypoint.py, because the model will be in the same folder as the script\n",
    "with open(\"../models/model.joblib\", 'rb') as model_file:\n",
    "    linear_reg = joblib.load(model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create example payloads\n",
    "\n",
    "In this section, we are preparing example payloads that will be used to test our inference model.  \n",
    "The payload is provided by the `AI Inference Server`, and contains the input data from PROFINET IO as a list of dictionaries in this example.\n",
    "\n",
    "First, we create `data_dicts` as is a list of dictionaries, where each dictionary represents a row in the original DataFrame.  \n",
    "Each payload is a subset of the `data_dicts` list, containing an arbitrary chosen number of consecutive dictionaries (in this case, it's 60, so each payload contain exactly one minute of measurements).  \n",
    "The `payloads` are created by taking slices of these dictionaries at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "data = pandas.read_csv(\"../data/historical_data.csv\")\n",
    "\n",
    "input_tags = [\"temperature_A\", \"temperature_B\", \"temperature_C\", \"valve_position_A\", \"valve_position_B\"]\n",
    "data = data[input_tags]\n",
    "\n",
    "data_dicts = data.to_dict(orient='records')\n",
    "\n",
    "payloads = [data_dicts[i:i + 60] for i in range(0, len(data_dicts), 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing input payloads\n",
    "\n",
    "We define a `process_input(...)` function that acts as the entry point. The method uses our model to predict the pH value on each record found in a payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(payload: list):\n",
    "    \n",
    "    for record in payload:  # record is a dictionary with the keys being the input_tags\n",
    "        df = pandas.DataFrame(record, index=[0])\n",
    "        phC = linear_reg.predict(df)\n",
    "        print(phC)\n",
    "\n",
    "process_input(payloads[0])  # processing the first payload which contains 1 minute of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating valve control values\n",
    "\n",
    "Based on the previous script, we expand the `process_input(...)` method so that the valve positions are also calculated based on the predictions of the model.\n",
    "\n",
    "First, we define an expected pH value for container C as `phC_mean = 9.0`. Once the `process_input(...)` method is called with a payload, it calculates the average predicted pH value and compares it to the expected value by a ratio called the `flow_control_ratio`.  \n",
    "Based on the value of this ratio, we adjust the angle positions of the valves in opposite directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phC_mean = 9.0\n",
    "\n",
    "def process_input(payload: list):\n",
    "    \n",
    "    phC_predictions = []\n",
    "    for record in payload:  \n",
    "        # calculating the predicted phC for each record in the payload\n",
    "        df = pandas.DataFrame(record, index=[0])\n",
    "        phC_predictions.append(linear_reg.predict(df))\n",
    "\n",
    "    phC_predictions = np.array(phC_predictions).flatten()  # create a 1D numpy array\n",
    "    flow_control_ratio = phC_predictions.mean() / phC_mean  # calculate the flow control ratio\n",
    "\n",
    "    valve_position_A_values = np.array(df['valve_position_A']).flatten()\n",
    "    valve_position_A = valve_position_A_values.mean() * flow_control_ratio  # adjust the valve position A\n",
    "\n",
    "    valve_position_B_values = np.array(df['valve_position_B']).flatten()\n",
    "    valve_position_B = valve_position_B_values.mean() / flow_control_ratio  # adjust the valve position B\n",
    "\n",
    "    return {\"valve_position_A\": valve_position_A, \"valve_position_B\": valve_position_B, \"predicted_phC\": phC_predictions.mean()}\n",
    "\n",
    "print(process_input(payloads[1])) # processing the second payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return object of the `process_input(..)` must be a dictionary, containing the variables we want to provide for `AI Inference Server` as an output.  \n",
    "This way `AI Inference Server` will be able to forward the data to the mapped `Data Connectors`.\n",
    "\n",
    "The Python script [entrypoint.py](../src/inference/entrypoint.py) serves as the entrypoint for the pipeline we are going to build in the [30-CreatePipelinePackage](30-CreatePipelinePackage.ipynb) notebook.  \n",
    "The python script contains the `process_input(...)` method, as well as as the necessary imports and loading our trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (soft_sensor_tutorial)",
   "language": "python",
   "name": "soft_sensor_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
