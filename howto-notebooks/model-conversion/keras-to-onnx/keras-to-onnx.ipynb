{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "SPDX-FileCopyrightText": "Copyright (C) Siemens AG 2021. All Rights Reserved.",
		    "SPDX-License-Identifier": "MIT"
      },
      "source": [
        "# Model Conversion for Keras models\n",
        "\n",
        "This tutorial explains how to convert a Keras Model stored in tensorflow's 'h5' format to a standardized ONNX format, which enables you to run your model on a GPU enabled AI Inference Server.  \n",
        "\n",
        "In this tutorial you learn how to \n",
        "- load a model in 'h5' format\n",
        "- convert and save the loaded model into 'onnx' format\n",
        "- verify the input and output shape of the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model\n",
        "\n",
        "For this model conversion tutorial we are going to use the same model we created and trained in our [Image Classification](\"../../use-cases/image-classification/Readme.md\") example.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "PYTHON_VERSION = sys.version_info\n",
        "\n",
        "if PYTHON_VERSION.minor == 11:\n",
        "    model_path = Path('./models/classification_mobilnet-py311.h5')\n",
        "else:\n",
        "    model_path = Path('./models/classification_mobilnet-py310.h5')\n",
        "\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert and Save\n",
        "\n",
        "In order to be able to convert your keras model to ONNX, you need to know the input shape and type of your model.  \n",
        "These are stored in the properties of the original model as:\n",
        "\n",
        "- `model.input.shape`, and\n",
        "- `model.input.dtype`\n",
        "\n",
        "Using these two, we can create an `input_signature` as `tensorflow.TensorSpec` class.  \n",
        "In the constructor of TensorSpec you can define the `name` of the input tensor. In this tutorial, we are setting the `name` of the input tensor as `input`.  \n",
        "Parameter `opset` defines the version of the `ONNX format`, opset version `13` refers to ONNX format `1.8.0` which is supported by AI Inference Server at the time of writing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tf2onnx\n",
        "import onnx\n",
        "import tensorflow as tf\n",
        "    \n",
        "if PYTHON_VERSION.minor == 11:\n",
        "    input_signature = [tf.TensorSpec(inp.shape, inp.dtype, name=f'input_{i}') for i, inp in enumerate(model.input)]\n",
        "else:\n",
        "    input_signature = [tf.TensorSpec(model.input.shape, model.input.dtype, name='input')]\n",
        "\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
        "onnx.save(onnx_model, \"./models/model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input and Output shape\n",
        "\n",
        "Let's inspect how the model and its inputs outputs are shaped.  \n",
        "`graph.input` displays the input shape of the converted ONNX model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "onnx_model.graph.input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the shape of the input is `[? x 224 x 224 x 3]` which is similar to the original `[None, 224, 224, 3]` but its first dimension is called `unk_334` here.  \n",
        "We can rename it defining the `dim_param` on the relevant dimension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "onnx_model.graph.input[0].type.tensor_type.shape.dim[0].dim_param = 'batch_size'\n",
        "onnx_model.graph.input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can do the same with the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Before:\\n\", onnx_model.graph.output)\n",
        "onnx_model.graph.output[0].type.tensor_type.shape.dim[0].dim_param = 'batch_size'\n",
        "print(\"After:\\n\", onnx_model.graph.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can save the model again, and try it out.  \n",
        "The model works with the original settings, but this way the two will be semantically linked and more readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "onnx.save(onnx_model, \"./models/model_renamed.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing the model\n",
        "\n",
        "Before packaging the model, it is recommended to try it out with the `onnxruntime` Python package.  \n",
        "To do so we need to provide  \n",
        "- an `onnxruntime.InferenceSession` with the preloaded model  \n",
        "- the dictionary of the `input` tensors with the expected shape and type.  \n",
        "  To test the model we are generating a numpy array with randomized float values. \n",
        "- the list of the `output` tensors,  \n",
        "  now it is the last layer of our tensorflow model with name `dense`  \n",
        "The shape can be changed by defining the variable `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "from onnxruntime import InferenceSession\n",
        "\n",
        "batch_size = 10\n",
        "images = numpy.random.random((batch_size, 224, 224, 3)).astype('float32')\n",
        "session = InferenceSession(\"./models/model.onnx\")\n",
        "\n",
        "if PYTHON_VERSION.minor == 11:\n",
        "    result = session.run([\"dense\"], {\"input_0\": images})\n",
        "else:\n",
        "    result = session.run([\"dense\"], {\"input\": images})\n",
        "    \n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `result` contains the output tensors in a list, in this case its the only one `dense` tensor.  \n",
        "To get the most likely class where the images belong to, we need to extract the first element of the list: `result[0]` and to iterate through the probabilities of the classes by image searching for the highest probability.  \n",
        "The shape of `result[0]` tensor is  `batch_size x 5` where `5` is the number of our labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = ['Label 101', 'Label 102', 'Label 103', 'Label 104', 'Label 105']\n",
        "for probabilities in result[0]:\n",
        "    class_index = numpy.argmax(probabilities)\n",
        "    print(f\"predicted class: {labels[class_index]}\\n  probabilities {probabilities}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage of the ONNX model\n",
        "\n",
        "The AI Inference Server with GPU support accepts ONNX models for execution.  \n",
        "For this purpose the model must be packaged into a `GPURuntimeComponent` step using AI Software Development Kit.  \n",
        "For details on how to create `GPURuntimeComponent` and build pipelines that run on a GPU enabled AI Inference Server you can study the [Object Detection](\"../../use-cases/object-detection/Readme.md\") example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using custom layers in your model\n",
        "\n",
        "AI Inference Server supports the execution of models containing custom layers.\\\n",
        "In case of using TensorRT optimization in the model configuraton, AI Inference Server attempts to run the model on the TensorRT backend. In case of an unrecognized operation (custom layers), the given operation will be executed on ONNX runtime backend.\\\n",
        "This could result in performance degradation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "keras",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
